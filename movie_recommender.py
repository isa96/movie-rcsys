# -*- coding: utf-8 -*-
"""Movie_Recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171XUqfzpM-GGZJTB53Tm2Ct8dEdXLqlf

# Import All Libraries
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""# Read All Datasets"""

movie_titles = pd.read_csv('/content/Movie_Id_Titles.csv')
users = pd.read_csv('/content/Dataset.csv')

movie_titles.head()

users.head()

"""# Exploratory data

## Exploratory Data for Movie Titles
"""

movie_titles.info()

movie_titles.title.unique()

print("There are {} movies".format(len(movie_titles['title'].unique())))

"""## Exploratory Data for Users"""

users.info()

users.describe()

print("There are {} users".format(len(users['user_id'].unique())))

"""# Data Preprocessing

##Merge Movie Titles file and Users File
"""

data = pd.merge(movie_titles, users, on="item_id", how="left")
data.head()

"""# Data Preparation

Check missing value
"""

data.isnull().sum()

"""Because there is no missing value, we can move to the next step

Sort all data based on item_id and put them into a fix_data variable
"""

fix_data = data.sort_values('item_id', ascending=True)
fix_data.head()

print("There are {} data in fix_data".format(len(fix_data['item_id'].unique())))

"""Check all titles in data"""

fix_data.title.unique()

"""Check movie with title: Toy Story (1995)"""

fix_data[fix_data['title']== 'Toy Story (1995)']

"""Make a preparation variable from fix_data and sort based on item_id"""

preparation = fix_data
preparation.sort_values('item_id')

preparation = preparation.drop_duplicates('item_id')
preparation

"""Convert item_id and title into list"""

item_id = preparation['item_id'].tolist()
title = preparation['title'].tolist()

"""Create dictionary for item_id and title"""

data_new = pd.DataFrame({
    'item_id': item_id,
    'title': title
})

data_new

"""# Modeling with Content Based Filtering"""

data_new.sample(5)

"""## TF-IDF Vectorizer"""

tf = TfidfVectorizer()
tf.fit(data_new['title']) 
tf.get_feature_names_out(['title'])

"""Fit transform into matrix"""

tfidf_matrix = tf.fit_transform(data_new['title'])
tfidf_matrix.shape

"""1682 is a data and 2429 is matrix for book title """

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names_out(),
    index=data_new.title
).sample(22, axis=1).sample(10, axis=0)

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""Make a dataframe from cosine_sim with row and colomn's name is title"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data_new['title'], columns=data_new['title'])
print('Shape:', cosine_sim_df.shape)
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## Make a Recommendation"""

def movie_recommendations(title, similarity_data=cosine_sim_df, items=data_new[['item_id', 'title']], k=5):

    # take data using argpartition to partition indirectly from the axis that given    
    # Change dataframe into numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))
    
    #Take the bigest similarity data from avaliable index
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop book title therefore book title that we type does not appear
    closest = closest.drop(title, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

movie_recommendations("Toy Story (1995)")

"""# Collaborative Filtering

## Encoded user_id
"""

# Change used_id into list without same value
user_id = preparation['user_id'].unique().tolist()
# user_id encoding
user_to_user_encoded = {x: i for i, x in enumerate(user_id)}
# numeric endoding to user id
user_encoded_to_user = {i: x for i, x in enumerate(user_id)}

print('user_id list: ', user_id)
print('\nencoded user_id: ', user_to_user_encoded)
print('\nnumeric encoded into user_id', user_encoded_to_user)

"""## Encoded item_id"""

# Change used_id into list without same value
item_id = preparation['item_id'].unique().tolist()
# user_id encoding
item_to_item_encoded = {x: i for i, x in enumerate(item_id)}
# numeric endoding to user id
item_encoded_to_item = {i: x for i, x in enumerate(item_id)}

print('user_id list: ', item_id)
print('\nencoded user_id: ', item_to_item_encoded)
print('\nnumeric encoded into user_id', item_encoded_to_item)

"""Mapping user_id and item_id into dataframe"""

preparation['user'] = preparation['user_id'].map(user_to_user_encoded)
preparation['item'] = preparation['item_id'].map(item_to_item_encoded)

preparation.head()

#Get number of user
num_users = len(user_to_user_encoded)

#Get number of movie item
num_items = len(item_to_item_encoded)

#Change rating into float data type
preparation['rating'] = preparation['rating'].values.astype(np.float32)

#Get minimum rating
min_rating = min(preparation['rating'])

#Get maximum rating
max_rating = max(preparation['rating'])

print("Number of user: {} \nNumber of movie item: {} \nMinimum rating: {} \nMax Rating: {}".format(num_users, num_items, min_rating, max_rating))

preparation = preparation.sample(frac=1, random_state = 42)
preparation

"""##Split Dataset"""

x = preparation[['user', 'item']].values
y = preparation['rating'].apply(lambda x: (x-min_rating) / (max_rating - min_rating)).values

#divide 80% train and 20% test
train_indices = int(0.8 * preparation.shape[0])
x_train, x_test, y_train, y_test = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print("x train shape: {} \ny train shape: {}".format(x_train.shape, y_train.shape))
print("x test shape: {} \ny test shape: {}".format(x_test.shape, y_test.shape))

"""#Train Model"""

class RecommenderNet(tf.keras.Model):
 
  def __init__(self, num_users, num_items, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_items = num_items
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.item_embedding = layers.Embedding( # layer embeddings resto
        num_items,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.item_bias = layers.Embedding(num_items, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # Get layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) #Get layer embedding 2
    item_vector = self.item_embedding(inputs[:, 1]) # Get layer embedding 3
    item_bias = self.item_bias(inputs[:, 1]) # Get layer embedding 4
 
    dot_user_item = tf.tensordot(user_vector, item_vector, 2) 
 
    x = dot_user_item + user_bias + item_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_items, 50)
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_test, y_test)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#Get Recommendation for User"""

movie = data_new
# Get User Sample
user_id = users['user_id'].sample(1).iloc[0]
movie_watched_by_user = users[users['user_id'] == user_id]

movie_not_watched = movie[~movie['item_id'].isin(movie_watched_by_user['user_id'].values)]['item_id'] 
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(item_to_item_encoded.keys()))
)
 
movie_not_watched = [[item_to_item_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    item_encoded_to_item.get(movie_not_watched [x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)
 
top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .item_id.values
)
 
movie_df_rows = movie[movie['item_id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.item_id, ':', row.title)
 
print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)
 
recommended_movie = movie[movie['item_id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.item_id, ':', row.title)